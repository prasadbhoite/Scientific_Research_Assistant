# ğŸ§ª Scientific Research Assistant

The **Scientific Research Assistant** is an intelligent, interactive application designed to assist researchers, reviewers, and academics in streamlining the peer review and literature analysis process. This assistant provides functionalities such as literature summarization, peer review generation, and citation intelligence â€” all powered by large language models (LLMs).

---

## ğŸ” Core Functionalities

### 1. ğŸ“š Literature Summarization
- Upload or fetch papers (PDFs, DOIs, or PubMed links).
- Extract and summarize sections such as:
  - Abstract, Introduction, Methods, Results, Conclusion
- Accepts **custom prompts** like:
  - â€œSummarize limitationsâ€
  - â€œExtract statistical methodsâ€
- **Tools & Libraries**:
  - `PyMuPDF` or `pdfminer.six` for PDF parsing
  - `LangChain` or `LlamaIndex` for chunking, embedding, and retrieval

---

### 2. ğŸ§‘â€âš–ï¸ Peer Review Assistant
- Accepts full manuscript or preprint.
- Generates section-wise critique:
  - Highlights strengths, weaknesses, and suggestions
- Customizable tone: `Neutral`, `Critical`, or `Supportive`
- Supports checklist-based review (e.g., **CONSORT**, **PRISMA**)

---

### 3. ğŸ”— Citation Analysis
- Extract and list all references from the manuscript.
- For each citation:
  - Check **recency**, **impact factor**, and **PubMed status**
  - Analyze author and regional diversity
  - Identify missing recent/relevant literature
- **APIs & Tools**:
  - `CrossRef API`, `Semantic Scholar API`, `OpenAlex`, `PubMed E-utilities`

---

## ğŸ§° Suggested Tech Stack

### Frontend
- ğŸ§ª **Streamlit** for rapid MVP prototyping
- âš›ï¸ **React + FastAPI** for production deployments

### Backend
- ğŸ” `LangChain`, `LlamaIndex`, or `Haystack` for LLM integration
- ğŸ“ `FAISS` or `ChromaDB` for vector store and citation retrieval
- ğŸ—„ï¸ `PostgreSQL` or `SQLite` for metadata and storage

### Language Models
- ğŸ”¥ **OpenAI GPT-4o** or **GPT-4-turbo**
- ğŸ§  **Claude 3** (Anthropic) for high-fidelity summarization
- ğŸ¦™ **Mistral** or **LLaMA3** for private/local model deployment

---

## ğŸš€ Project Roadmap

### âœ… Phase 1 (MVP): Local PDF Summarizer
- Upload scientific paper (PDF)
- Output structured summary (IMRaD format)
- Highlight key findings & limitations

### âœ… Phase 2: Peer Review Generator
- Add review input fields: journal name, tone, length
- Generate full-text peer review
- Option to batch upload multiple papers

### âœ… Phase 3: Citation Intelligence
- Extract and analyze references from PDF
- Enrich metadata via APIs
- Flag:
  - Self-citations
  - Missing landmark papers
  - Weak or outdated references

### âœ… Phase 4: Export & Personalization
- Export output as `.docx` or `.pdf`
- Save and version control summaries & reviews
- Enable â€œreview style presetsâ€ (e.g., NIH reviewer, clinician, statistician)

---

## ğŸ”’ Optional Advanced Features
- On-device LLMs via **Ollama**, **Mixtral**, or **LLaMA3**
- Chat with a research paper using **RAG** over vectorized content
- Cross-paper comparison (for systematic reviews)
- Integration with **Zotero** or **EndNote** for citation syncing

---

## ğŸ“¦ Installation (Coming Soon)

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # or .venv\\Scripts\\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Set your API key
export OPENAI_API_KEY=your_key_here

# Run the app
streamlit run main.py
